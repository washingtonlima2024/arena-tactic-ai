import "https://deno.land/x/xhr@0.1.0/mod.ts";
import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
import { createClient } from "https://esm.sh/@supabase/supabase-js@2";

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
};

// Format seconds to SRT timestamp format (HH:MM:SS,mmm)
function formatSrtTimestamp(seconds: number): string {
  const hours = Math.floor(seconds / 3600);
  const minutes = Math.floor((seconds % 3600) / 60);
  const secs = Math.floor(seconds % 60);
  const ms = Math.round((seconds % 1) * 1000);
  
  return `${hours.toString().padStart(2, '0')}:${minutes.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')},${ms.toString().padStart(3, '0')}`;
}

// Convert Whisper segments to SRT format
function segmentsToSrt(segments: Array<{ start: number; end: number; text: string }>): string {
  return segments
    .map((segment, index) => {
      const startTime = formatSrtTimestamp(segment.start);
      const endTime = formatSrtTimestamp(segment.end);
      return `${index + 1}\n${startTime} --> ${endTime}\n${segment.text.trim()}\n`;
    })
    .join('\n');
}

serve(async (req) => {
  if (req.method === 'OPTIONS') {
    return new Response(null, { headers: corsHeaders });
  }

  try {
    const { matchId, videoUrl, embedUrl } = await req.json();

    console.log('Starting audio extraction for match:', matchId);
    console.log('Video URL:', videoUrl || embedUrl);

    const OPENAI_API_KEY = Deno.env.get('OPENAI_API_KEY');
    const SUPABASE_URL = Deno.env.get('SUPABASE_URL')!;
    const SUPABASE_SERVICE_ROLE_KEY = Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!;

    if (!OPENAI_API_KEY) {
      throw new Error('OPENAI_API_KEY is not configured');
    }

    const supabase = createClient(SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY);

    // For embed URLs, we need to use a different approach
    // Since we can't directly access video from embed, we'll use AI to analyze
    // and generate timestamps based on the analysis
    
    // Check if we have actual video file
    const sourceUrl = videoUrl || embedUrl;
    
    if (!sourceUrl) {
      throw new Error('No video source provided');
    }

    // Try to transcribe using Whisper with verbose timestamps
    // For embeds, we'll generate synthetic SRT from AI analysis
    let srtContent = '';
    let transcribedText = '';

    // Check if it's an embed URL (can't directly process)
    const isEmbed = sourceUrl.includes('embed') || sourceUrl.includes('iframe') || sourceUrl.includes('xtream');

    if (isEmbed) {
      console.log('Embed detected - generating analysis-based SRT');
      
      // Use AI to generate a structured match narration with timestamps
      // This will be based on match analysis
      const { data: events } = await supabase
        .from('match_events')
        .select('*')
        .eq('match_id', matchId)
        .order('minute', { ascending: true });

      if (events && events.length > 0) {
        // Generate SRT from events
        const segments = events.map((event, index) => {
          const startSeconds = (event.minute || 0) * 60 + (event.second || 0);
          const endSeconds = startSeconds + 10; // 10 second segments
          
          let eventText = '';
          switch (event.event_type) {
            case 'goal':
              eventText = `‚öΩ GOL! ${event.description || 'Gol marcado!'}`;
              break;
            case 'shot':
              eventText = `üéØ Finaliza√ß√£o - ${event.description || 'Chute a gol'}`;
              break;
            case 'foul':
              eventText = `‚ö†Ô∏è Falta - ${event.description || 'Falta marcada'}`;
              break;
            case 'card':
              eventText = `üü® Cart√£o - ${event.description || 'Cart√£o mostrado'}`;
              break;
            case 'offside':
              eventText = `üö© Impedimento - ${event.description || 'Jogador em impedimento'}`;
              break;
            case 'corner':
              eventText = `üîÑ Escanteio - ${event.description || 'Escanteio cobrado'}`;
              break;
            default:
              eventText = event.description || `Evento: ${event.event_type}`;
          }
          
          return {
            start: startSeconds,
            end: endSeconds,
            text: `[${event.minute}'${event.second ? ':' + event.second.toString().padStart(2, '0') : ''}] ${eventText}`
          };
        });

        srtContent = segmentsToSrt(segments);
        transcribedText = events.map(e => e.description || e.event_type).join('. ');
        
        console.log('Generated SRT from', events.length, 'events');
      } else {
        // No events yet - create placeholder SRT
        srtContent = `1\n00:00:00,000 --> 00:00:30,000\n[In√≠cio da partida]\n\n2\n00:45:00,000 --> 00:45:30,000\n[Intervalo - Fim do 1¬∫ tempo]\n\n3\n00:45:30,000 --> 00:46:00,000\n[In√≠cio do 2¬∫ tempo]\n\n4\n01:30:00,000 --> 01:30:30,000\n[Fim da partida]\n`;
        transcribedText = 'Partida aguardando an√°lise de v√≠deo';
      }
    } else {
      // Direct video file - use Whisper for transcription
      console.log('Processing direct video file with Whisper');
      
      try {
        // Fetch video file
        const videoResponse = await fetch(sourceUrl);
        if (!videoResponse.ok) {
          throw new Error(`Failed to fetch video: ${videoResponse.status}`);
        }
        
        const videoBuffer = await videoResponse.arrayBuffer();
        console.log('Video size:', videoBuffer.byteLength, 'bytes');
        
        // Create form data for Whisper with verbose_json for timestamps
        const formData = new FormData();
        const videoBlob = new Blob([videoBuffer], { type: 'video/mp4' });
        formData.append('file', videoBlob, 'video.mp4');
        formData.append('model', 'whisper-1');
        formData.append('language', 'pt');
        formData.append('response_format', 'verbose_json');
        formData.append('timestamp_granularities[]', 'segment');

        const whisperResponse = await fetch('https://api.openai.com/v1/audio/transcriptions', {
          method: 'POST',
          headers: {
            'Authorization': `Bearer ${OPENAI_API_KEY}`,
          },
          body: formData,
        });

        if (!whisperResponse.ok) {
          const errorText = await whisperResponse.text();
          console.error('Whisper API error:', errorText);
          throw new Error(`Whisper transcription failed: ${errorText}`);
        }

        const whisperResult = await whisperResponse.json();
        console.log('Whisper result segments:', whisperResult.segments?.length || 0);

        if (whisperResult.segments && whisperResult.segments.length > 0) {
          srtContent = segmentsToSrt(whisperResult.segments);
          transcribedText = whisperResult.text;
        } else {
          srtContent = `1\n00:00:00,000 --> 00:00:30,000\n${whisperResult.text || 'Transcri√ß√£o n√£o dispon√≠vel'}\n`;
          transcribedText = whisperResult.text || '';
        }
      } catch (videoError) {
        console.error('Video processing error:', videoError);
        // Fallback to placeholder
        srtContent = `1\n00:00:00,000 --> 00:00:30,000\n[√Åudio n√£o dispon√≠vel - an√°lise visual apenas]\n`;
        transcribedText = 'Transcri√ß√£o n√£o dispon√≠vel';
      }
    }

    console.log('SRT content generated, length:', srtContent.length);

    return new Response(JSON.stringify({ 
      success: true,
      srtContent,
      transcribedText,
      method: isEmbed ? 'events_based' : 'whisper',
    }), {
      headers: { ...corsHeaders, 'Content-Type': 'application/json' },
    });

  } catch (error) {
    console.error('Error in extract-audio-srt:', error);
    return new Response(JSON.stringify({ 
      success: false,
      error: error instanceof Error ? error.message : 'Unknown error occurred' 
    }), {
      status: 500,
      headers: { ...corsHeaders, 'Content-Type': 'application/json' },
    });
  }
});
